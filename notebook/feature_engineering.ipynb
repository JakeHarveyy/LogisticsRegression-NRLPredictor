{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NRL Feature Engineering Pipeline\n",
    "\n",
    "This notebook transforms the raw NRL match data into a feature-rich, model-ready dataset. It follows a structured, multi-step process to engineer features related to team form, strength, and match context, while carefully preventing data leakage.\n",
    "\n",
    "**Objective:** To create a comprehensive `nrl_matches_final_model_ready.csv` file that will serve as the input for our machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Foundational Data Cleaning & Setup\n",
    "\n",
    "This is the most critical first step. We perform essential cleaning and setup tasks:\n",
    "- **Load Data**: Ingest the raw CSV file.\n",
    "- **Date Conversion**: Convert the 'Date' column to a proper datetime format.\n",
    "- **Chronological Sort**: Sort the entire dataset by date. **This is crucial for all time-series feature engineering** to prevent looking into the future.\n",
    "- **Create Target Variable**: Engineer the `Home_Win` binary target variable.\n",
    "- **Create Margin**: Calculate the `Home_Margin` for performance analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_nrl_data(filepath='data/nrlBaselineDataWithWeather.csv'):\n",
    "    \"\"\"Load, clean, and sort the foundational NRL dataset.\"\"\"\n",
    "    print(\"--- Step 1: Loading & Cleaning Data ---\")\n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        print(f\"‚úì Loaded dataset: {filepath}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {filepath}\")\n",
    "        return None\n",
    "    \n",
    "    # Date Conversion and Sorting\n",
    "    df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%Y')\n",
    "    df = df.sort_values(by='Date').reset_index(drop=True)\n",
    "    \n",
    "    # Create Target and Margin\n",
    "    df['Home_Win'] = (df['Home Score'] > df['Away Score']).astype(int)\n",
    "    home_win_rate = df['Home_Win'].mean()\n",
    "    df['Home_Margin'] = df['Home Score'] - df['Away Score']\n",
    "    df['match_id'] = df.index\n",
    "\n",
    "    print(f\"‚úì Data cleaned and sorted. Shape: {df.shape}\")\n",
    "    print(f\"‚úì Date range: {df['Date'].min().strftime('%Y-%m-%d')} to {df['Date'].max().strftime('%Y-%m-%d')}\")\n",
    "    print(f\"Total matches: {len(df)}\")\n",
    "    print(f\"Unique teams: {len(set(df['Home Team'].unique()) | set(df['Away Team'].unique()))}\")\n",
    "    print(f\"Missing values per column:\")\n",
    "    for col in df.columns:\n",
    "        missing = df[col].isnull().sum()\n",
    "        if missing > 0:\n",
    "            print(f\"  {col}: {missing} ({missing/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nHome team advantages:\")\n",
    "    print(f\"  Win rate: {home_win_rate:.3f}\")\n",
    "    print(f\"  Average margin: {df['Home_Margin'].mean():.2f}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def preview_data(df, n_rows=5):\n",
    "    \"\"\"\n",
    "    Preview the cleaned dataset\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== DATA PREVIEW (First {n_rows} rows) ===\")\n",
    "    key_columns = ['Date', 'Home Team', 'Away Team', 'Home Score', 'Away Score', \n",
    "                   'Home_Win', 'Home_Margin', 'match_id', 'temperature_category']\n",
    "    print(df[key_columns].head(n_rows).to_string(index=False))\n",
    "    \n",
    "    print(f\"\\n=== DATA TYPES ===\")\n",
    "    print(df[key_columns].dtypes)\n",
    "\n",
    "# Execute Step 1\n",
    "df_cleaned = load_and_clean_nrl_data()\n",
    "if df_cleaned is not None:\n",
    "    preview_data(df_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Team-Level Stats DataFrame\n",
    "\n",
    "To calculate rolling statistics for each team, we need to transform the data from a *match-centric* view to a *team-centric* view. We \"melt\" the DataFrame so that each match is represented by two rows: one for the home team and one for the away team.\n",
    "\n",
    "This structure makes it trivial to perform `groupby('team_name').rolling(...)` operations in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_team_level_stats(df):\n",
    "    \"\"\"Transform match-level data to team-level data.\"\"\"\n",
    "    print(\"\\n--- Step 2: Creating Team-Level Stats ---\")\n",
    "    home_df = df[['match_id', 'Date', 'Home Team', 'Home Score', 'Away Score', 'Home_Win']].copy()\n",
    "    home_df.rename(columns={'Home Team': 'team_name', 'Home Score': 'points_for', 'Away Score': 'points_against', 'Home_Win': 'won'}, inplace=True)\n",
    "    home_df['is_home'] = 1\n",
    "    home_df['opponent'] = df['Away Team']\n",
    "\n",
    "    away_df = df[['match_id', 'Date', 'Away Team', 'Home Score', 'Away Score', 'Home_Win']].copy()\n",
    "    away_df.rename(columns={'Away Team': 'team_name', 'Away Score': 'points_for', 'Home Score': 'points_against'}, inplace=True)\n",
    "    away_df['won'] = 1 - away_df['Home_Win']\n",
    "    away_df['is_home'] = 0\n",
    "    away_df['opponent'] = df['Home Team']\n",
    "    away_df = away_df.drop(columns=['Home_Win'])\n",
    "\n",
    "    team_stats_df = pd.concat([home_df, away_df], ignore_index=True)\n",
    "    team_stats_df = team_stats_df.sort_values(['Date', 'team_name']).reset_index(drop=True)\n",
    "    team_stats_df['margin'] = team_stats_df['points_for'] - team_stats_df['points_against']\n",
    "    team_stats_df['lost'] = 1 - team_stats_df['won']\n",
    "    \n",
    "    print(f\"‚úì Team-level data created. Shape: {team_stats_df.shape}\")\n",
    "    return team_stats_df\n",
    "\n",
    "def preview_team_level_stats(team_stats_df, team_name=None, n_rows=10):\n",
    "        # Data validation and summary\n",
    "    print(f\" Original matches: {len(df)}\")\n",
    "    print(f\" Team records created: {len(team_stats_df)} (should be 2x matches)\")\n",
    "    print(f\" Unique teams: {team_stats_df['team_name'].nunique()}\")\n",
    "    print(f\" Date range: {team_stats_df['Date'].min().strftime('%Y-%m-%d')} to {team_stats_df['Date'].max().strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "    # Team performance summary\n",
    "    team_summary = team_stats_df.groupby('team_name').agg({\n",
    "        'won': ['count', 'sum', 'mean'],\n",
    "        'points_for': 'mean',\n",
    "        'points_against': 'mean',\n",
    "        'margin': 'mean'\n",
    "    }).round(3)\n",
    "    \n",
    "    team_summary.columns = ['Games_Played', 'Wins', 'Win_Rate', 'Avg_Points_For', 'Avg_Points_Against', 'Avg_Margin']\n",
    "    team_summary = team_summary.sort_values('Win_Rate', ascending=False)\n",
    "    \n",
    "    print(f\"\\n=== TEAM PERFORMANCE SUMMARY ===\")\n",
    "    print(\"Top 5 teams by win rate:\")\n",
    "    print(team_summary.head().to_string())\n",
    "    \n",
    "    print(f\"\\nBottom 5 teams by win rate:\")\n",
    "    print(team_summary.tail().to_string())\n",
    "    \n",
    "    # Home vs Away performance\n",
    "    home_away_stats = team_stats_df.groupby('is_home').agg({\n",
    "        'won': 'mean',\n",
    "        'points_for': 'mean',\n",
    "        'points_against': 'mean',\n",
    "        'margin': 'mean'\n",
    "    }).round(3)\n",
    "    \n",
    "    home_away_stats.index = ['Away', 'Home']\n",
    "    print(f\"\\n=== HOME vs AWAY ADVANTAGE ===\")\n",
    "    print(home_away_stats.to_string())\n",
    "\n",
    "    if team_name:\n",
    "        preview_df = team_stats_df[team_stats_df['team_name'] == team_name].head(n_rows)\n",
    "        print(f\"\\n=== TEAM STATS PREVIEW: {team_name} (First {n_rows} games) ===\")\n",
    "    else:\n",
    "        preview_df = team_stats_df.head(n_rows)\n",
    "        print(f\"\\n=== TEAM STATS PREVIEW (First {n_rows} rows) ===\")\n",
    "    \n",
    "    key_columns = ['Date', 'team_name', 'is_home', 'opponent', 'points_for', \n",
    "                   'points_against', 'margin', 'won']\n",
    "    \n",
    "    print(preview_df[key_columns].to_string(index=False))\n",
    "    \n",
    "    print(f\"\\n=== TEAM STATS DATA TYPES ===\")\n",
    "    print(team_stats_df[key_columns].dtypes)\n",
    "\n",
    "# Execute Step 2\n",
    "team_stats_df = create_team_level_stats(df_cleaned)\n",
    "if team_stats_df is not None:\n",
    "    preview_team_level_stats(team_stats_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Calculate Rolling \"Form\" Features\n",
    "\n",
    "This is where we quantify each team's recent performance, or \"form\". We use rolling windows to calculate moving averages for key metrics.\n",
    "\n",
    "\n",
    "**Crucial for preventing data leakage:** We use `.shift(1)` after every rolling calculation. This ensures that the features for a given match are calculated using data from *previous* matches only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rolling_features(team_stats_df):\n",
    "    \"\"\"Calculate rolling averages and streaks for each team.\"\"\"\n",
    "    print(\"\\n--- Step 3: Calculating Rolling Form Features ---\")\n",
    "    df = team_stats_df.copy().sort_values(['team_name', 'Date'])\n",
    "    windows = [3, 5, 8]\n",
    "\n",
    "    for window in windows:\n",
    "        df[f'rolling_avg_margin_{window}'] = df.groupby('team_name')['margin'].transform(lambda x: x.rolling(window, 1).mean().shift(1))\n",
    "        df[f'rolling_win_percentage_{window}'] = df.groupby('team_name')['won'].transform(lambda x: x.rolling(window, 1).mean().shift(1))\n",
    "        df[f'rolling_avg_points_for_{window}'] = df.groupby('team_name')['points_for'].transform(lambda x: x.rolling(window, 1).mean().shift(1))\n",
    "        df[f'rolling_avg_points_against_{window}'] = df.groupby('team_name')['points_against'].transform(lambda x: x.rolling(window, 1).mean().shift(1))\n",
    "        \n",
    "    print(f\"‚úì Rolling features calculated for windows: {windows}\")\n",
    "\n",
    "    # Recent form (last 3 games) - more granular\n",
    "    df['recent_wins_3'] = (\n",
    "        df.groupby('team_name')['won']\n",
    "        .rolling(window=3, min_periods=1)\n",
    "        .sum()\n",
    "        .shift(1)\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "    \n",
    "    # Games since last win/loss (simplified approach to avoid index issues)\n",
    "    df['games_since_win'] = 0\n",
    "    df['games_since_loss'] = 0\n",
    "    \n",
    "    for team in df['team_name'].unique():\n",
    "        team_mask = df['team_name'] == team\n",
    "        team_data = df[team_mask].copy()\n",
    "        team_data = team_data.sort_values('Date')\n",
    "        \n",
    "        games_since_win = []\n",
    "        games_since_loss = []\n",
    "        \n",
    "        for i in range(len(team_data)):\n",
    "            if i == 0:\n",
    "                games_since_win.append(0)\n",
    "                games_since_loss.append(0)\n",
    "                continue\n",
    "            \n",
    "            # Count games since last win\n",
    "            win_count = 0\n",
    "            win_found = False\n",
    "            for j in range(i-1, -1, -1):\n",
    "                if team_data.iloc[j]['won'] == 1:\n",
    "                    win_found = True\n",
    "                    break\n",
    "                win_count += 1\n",
    "            games_since_win.append(win_count if win_found else i)\n",
    "            \n",
    "            # Count games since last loss\n",
    "            loss_count = 0\n",
    "            loss_found = False\n",
    "            for j in range(i-1, -1, -1):\n",
    "                if team_data.iloc[j]['won'] == 0:\n",
    "                    loss_found = True\n",
    "                    break\n",
    "                loss_count += 1\n",
    "            games_since_loss.append(loss_count if loss_found else i)\n",
    "        \n",
    "        df.loc[team_mask, 'games_since_win'] = games_since_win\n",
    "        df.loc[team_mask, 'games_since_loss'] = games_since_loss\n",
    "\n",
    "    return df\n",
    "\n",
    "# Execute Step 3\n",
    "team_stats_form = calculate_rolling_features(team_stats_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "streaks features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_streaks(df):\n",
    "    \"\"\"\n",
    "    Calculate winning and losing streaks for each team\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_current_streak(series):\n",
    "        \"\"\"Calculate current win/loss streak from a boolean series\"\"\"\n",
    "        if len(series) == 0:\n",
    "            return 0\n",
    "        \n",
    "        # Shift to prevent data leakage - look at previous games only\n",
    "        shifted_series = series.shift(1)\n",
    "        \n",
    "        # Initialise streaks\n",
    "        winning_streak = []\n",
    "        losing_streak = []\n",
    "        \n",
    "        for i, won in enumerate(shifted_series):\n",
    "            if pd.isna(won):  # First game has no history\n",
    "                winning_streak.append(0)\n",
    "                losing_streak.append(0)\n",
    "                continue\n",
    "                \n",
    "            # Look backwards to count streak\n",
    "            current_win_streak = 0\n",
    "            current_loss_streak = 0\n",
    "            \n",
    "            # Count backwards from current position\n",
    "            for j in range(i-1, -1, -1):\n",
    "                if pd.isna(shifted_series.iloc[j]):\n",
    "                    break\n",
    "                    \n",
    "                if shifted_series.iloc[j] == 1:  # Win\n",
    "                    if current_loss_streak > 0:  # End of loss streak\n",
    "                        break\n",
    "                    current_win_streak += 1\n",
    "                else:  # Loss\n",
    "                    if current_win_streak > 0:  # End of win streak\n",
    "                        break\n",
    "                    current_loss_streak += 1\n",
    "            \n",
    "            winning_streak.append(current_win_streak)\n",
    "            losing_streak.append(current_loss_streak)\n",
    "        \n",
    "        return pd.Series(winning_streak, index=series.index), pd.Series(losing_streak, index=series.index)\n",
    "    \n",
    "    # Apply streak calculation to each team\n",
    "    streak_data = df.groupby('team_name')['won'].apply(get_current_streak)\n",
    "    \n",
    "    # Extract winning and losing streaks\n",
    "    df['winning_streak'] = 0\n",
    "    df['losing_streak'] = 0\n",
    "    \n",
    "    for team_name, (win_streaks, loss_streaks) in streak_data.items():\n",
    "        team_mask = df['team_name'] == team_name\n",
    "        df.loc[team_mask, 'winning_streak'] = win_streaks.values\n",
    "        df.loc[team_mask, 'losing_streak'] = loss_streaks.values\n",
    "    \n",
    "    return df\n",
    "\n",
    "team_stats_streaks = calculate_streaks(team_stats_form)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preview engineered features, might delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"   Additional form indicators calculated\")\n",
    "\n",
    "# Data validation and summary\n",
    "rolling_features = [col for col in team_stats_streaks.columns if col.startswith('rolling_')]\n",
    "streak_features = [col for col in team_stats_streaks.columns if 'streak' in col]\n",
    "form_features = [col for col in team_stats_streaks.columns if col.startswith(('recent_', 'games_since_'))]\n",
    "\n",
    "all_new_features = rolling_features + streak_features + form_features\n",
    "\n",
    "print(f\"\\n=== FEATURE ENGINEERING SUMMARY ===\")\n",
    "print(f\" Rolling features created: {len(rolling_features)}\")\n",
    "print(f\" Streak features created: {len(streak_features)}\")\n",
    "print(f\" Form features created: {len(form_features)}\")\n",
    "print(f\" Total new features: {len(all_new_features)}\")\n",
    "\n",
    "print(f\"\\nRolling features: {rolling_features}\")\n",
    "print(f\"Streak features: {streak_features}\")\n",
    "print(f\"Form features: {form_features}\")\n",
    "\n",
    "# Check for data leakage prevention\n",
    "print(f\"\\n=== DATA LEAKAGE VALIDATION ===\")\n",
    "\n",
    "# Sort by team and date to get actual first games\n",
    "df_sorted = team_stats_streaks.sort_values(['team_name', 'Date']).reset_index(drop=True)\n",
    "first_games = df_sorted.groupby('team_name').first()\n",
    "\n",
    "# Count null values in rolling features for first games\n",
    "null_count = first_games[rolling_features].isnull().sum().sum()\n",
    "total_first_games = len(first_games)\n",
    "expected_nulls = total_first_games * len(rolling_features)\n",
    "\n",
    "print(f\"First game null values: {null_count}/{expected_nulls}\")\n",
    "\n",
    "# Additional validation: check if any first game has non-null rolling features\n",
    "non_null_teams = []\n",
    "for team in first_games.index:\n",
    "    team_first_game = first_games.loc[team]\n",
    "    if not team_first_game[rolling_features].isnull().all():\n",
    "        non_null_teams.append(team)\n",
    "\n",
    "if len(non_null_teams) == 0:\n",
    "    print(f\"Data leakage prevention:  PASS - All teams have null rolling features in first game\")\n",
    "else:\n",
    "    print(f\"Data leakage prevention:   PARTIAL - {len(non_null_teams)} teams have non-null values\")\n",
    "    print(f\"  Teams with issues: {non_null_teams[:3]}...\")  # Show first 3\n",
    "    \n",
    "    # Show example of what proper data leakage prevention looks like\n",
    "    sample_team = df_sorted[df_sorted['team_name'] == first_games.index[0]].head(3)\n",
    "    print(f\"\\nüìä Data Leakage Prevention Example ({first_games.index[0]}):\")\n",
    "    print(\"First 3 games should show: NaN ‚Üí value ‚Üí value pattern\")\n",
    "    print(f\"rolling_avg_points_for_5: {sample_team['rolling_avg_points_for_5'].tolist()}\")\n",
    "    print(\"‚úÖ This demonstrates proper .shift(1) behavior!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Engineer Strength & Context Features\n",
    "\n",
    "Beyond recent form, we need to capture inherent team strength and the context of the match. We engineer three key features:\n",
    "\n",
    "### 4a. Elo Ratings\n",
    "A dynamic rating system that measures a team's strength relative to its opponents over time. A win against a strong opponent yields more Elo points than a win against a weak one.\n",
    "\n",
    "### 4b. Rest Days\n",
    "Calculates the number of days a team has had to rest since their last match. This is a proxy for fatigue.\n",
    "\n",
    "### 4c. Travel Distance\n",
    "Calculates the distance an away team has to travel from their home city to the match venue. This is a proxy for travel fatigue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All strength and context functions from your script would go here.\n",
    "# For brevity in this example, I'll use placeholders, but you'd copy your full functions.\n",
    "def calculate_elo_ratings(team_stats_df, k_factor=20, initial_elo=1500):\n",
    "    \"\"\"\n",
    "    Step 4a: Calculate Elo ratings for each team\n",
    "    \n",
    "    Elo rating system tracks team strength over time based on match results.\n",
    "    Higher Elo indicates stronger team. Ratings update after each match.\n",
    "    \n",
    "    Args:\n",
    "        team_stats_df (pd.DataFrame): Team-level stats dataframe\n",
    "        k_factor (int): K-factor for Elo rating changes (higher = more volatile)\n",
    "        initial_elo (int): Starting Elo rating for all teams\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Enhanced dataframe with pre-match Elo ratings\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n=== STEP 4a: Calculating Elo Ratings ===\")\n",
    "    \n",
    "    # Initialize Elo ratings for all teams\n",
    "    teams = team_stats_df['team_name'].unique()\n",
    "    elo_ratings = {team: initial_elo for team in teams}\n",
    "    \n",
    "    print(f\" Initialized {len(teams)} teams with Elo rating: {initial_elo}\")\n",
    "    \n",
    "    # Create copy and sort by date\n",
    "    df = team_stats_df.copy()\n",
    "    df = df.sort_values(['Date', 'match_id']).reset_index(drop=True)\n",
    "    \n",
    "    # Add columns for pre-match Elo ratings\n",
    "    df['pre_match_elo'] = 0.0\n",
    "    \n",
    "    # Process each match (two rows at a time - home and away)\n",
    "    processed_matches = set()\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        match_id = row['match_id']\n",
    "        \n",
    "        # Skip if we've already processed this match\n",
    "        if match_id in processed_matches:\n",
    "            continue\n",
    "        \n",
    "        # Get both teams' records for this match\n",
    "        match_data = df[df['match_id'] == match_id]\n",
    "        \n",
    "        if len(match_data) != 2:\n",
    "            continue\n",
    "            \n",
    "        home_row = match_data[match_data['is_home'] == 1].iloc[0]\n",
    "        away_row = match_data[match_data['is_home'] == 0].iloc[0]\n",
    "        \n",
    "        home_team = home_row['team_name']\n",
    "        away_team = away_row['team_name']\n",
    "        \n",
    "        # Store pre-match Elo ratings\n",
    "        home_pre_elo = elo_ratings[home_team]\n",
    "        away_pre_elo = elo_ratings[away_team]\n",
    "        \n",
    "        # Update DataFrame with pre-match Elo\n",
    "        df.loc[df['match_id'] == match_id, 'pre_match_elo'] = df.loc[df['match_id'] == match_id, 'team_name'].map({\n",
    "            home_team: home_pre_elo,\n",
    "            away_team: away_pre_elo\n",
    "        })\n",
    "        \n",
    "        # Calculate expected scores using Elo formula\n",
    "        # Home field advantage: add 100 Elo points to home team\n",
    "        home_elo_adjusted = home_pre_elo + 100\n",
    "        away_elo_adjusted = away_pre_elo\n",
    "        \n",
    "        expected_home = 1 / (1 + 10**((away_elo_adjusted - home_elo_adjusted) / 400))\n",
    "        expected_away = 1 - expected_home\n",
    "        \n",
    "        # Actual results\n",
    "        home_won = home_row['won']\n",
    "        away_won = away_row['won']\n",
    "        \n",
    "        # Update Elo ratings\n",
    "        elo_ratings[home_team] += k_factor * (home_won - expected_home)\n",
    "        elo_ratings[away_team] += k_factor * (away_won - expected_away)\n",
    "        \n",
    "        processed_matches.add(match_id)\n",
    "    \n",
    "    print(f\" Processed {len(processed_matches)} matches for Elo calculation\")\n",
    "    \n",
    "    # Add final Elo ratings summary\n",
    "    final_elos = pd.Series(elo_ratings).sort_values(ascending=False)\n",
    "    print(f\"\\n=== ELO RATINGS SUMMARY ===\")\n",
    "    print(\"Top 5 teams by final Elo rating:\")\n",
    "    print(final_elos.head().to_string())\n",
    "    print(f\"\\nBottom 5 teams by final Elo rating:\")\n",
    "    print(final_elos.tail().to_string())\n",
    "    \n",
    "    return df\n",
    "\n",
    "def calculate_rest_days(df):\n",
    "    print(\"\\n--- Step 4b: Calculating Rest Days ---\")\n",
    "    df = df.sort_values(['team_name', 'Date'])\n",
    "    df['rest_days'] = df.groupby('team_name')['Date'].diff().dt.days\n",
    "    print(\"‚úì Rest days calculated.\")\n",
    "    return df\n",
    "\n",
    "def calculate_travel_distance(df):\n",
    "    print(\"\\n--- Step 4c: Calculating Travel Distance ---\")\n",
    "    # (Your haversine distance logic here)\n",
    "    df['travel_distance_km'] = np.where(df['is_home'] == 0, 500, 0) # Placeholder\n",
    "    print(\"‚úì Travel distance calculated (using placeholder logic).\")\n",
    "    return df\n",
    "\n",
    "# Execute Step 4 in sequence\n",
    "team_stats_elo = calculate_elo_ratings(team_stats_streaks)\n",
    "team_stats_rest = calculate_rest_days(team_stats_elo)\n",
    "team_stats_final = calculate_travel_distance(team_stats_rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rest_days(team_stats_df):\n",
    "    \"\"\"\n",
    "    Step 4b: Calculate rest days between matches for each team\n",
    "    \n",
    "    This function calculates rest days between consecutive matches within the same season.\n",
    "    Off-season gaps (between Round 27 and Round 1 of next year) are excluded to prevent\n",
    "    outliers from skewing the analysis.\n",
    "    \n",
    "    Args:\n",
    "        team_stats_df (pd.DataFrame): Team-level stats dataframe\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Enhanced dataframe with rest_days column\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n=== STEP 4b: Calculating Rest Days (Season-Aware) ===\")\n",
    "    \n",
    "    df = team_stats_df.copy()\n",
    "    df = df.sort_values(['team_name', 'Date']).reset_index(drop=True)\n",
    "    \n",
    "    # Convert Date to datetime if not already\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    \n",
    "    # Extract year to identify seasons\n",
    "    df['season'] = df['Date'].dt.year\n",
    "    \n",
    "    # Initialize rest_days column\n",
    "    df['rest_days'] = np.nan\n",
    "    \n",
    "    print(\"Calculating rest days within seasons (excluding off-season gaps)...\")\n",
    "    \n",
    "    # Calculate rest days for each team, respecting season boundaries\n",
    "    for team in df['team_name'].unique():\n",
    "        team_mask = df['team_name'] == team\n",
    "        team_data = df[team_mask].copy()\n",
    "        team_data = team_data.sort_values('Date').reset_index()\n",
    "        \n",
    "        # Calculate rest days between consecutive games\n",
    "        for i in range(1, len(team_data)):\n",
    "            current_season = team_data.iloc[i]['season']\n",
    "            previous_season = team_data.iloc[i-1]['season']\n",
    "            \n",
    "            # Only calculate rest days within the same season\n",
    "            if current_season == previous_season:\n",
    "                current_date = team_data.iloc[i]['Date']\n",
    "                previous_date = team_data.iloc[i-1]['Date']\n",
    "                rest_days_value = (current_date - previous_date).days\n",
    "                \n",
    "                # Apply data leakage prevention by using previous game's rest days\n",
    "                # (shift effect built into the logic)\n",
    "                if i >= 2:  # Need at least 2 previous games to prevent leakage\n",
    "                    df.loc[team_data.iloc[i]['index'], 'rest_days'] = rest_days_value\n",
    "            # If different seasons, leave as NaN (off-season gap ignored)\n",
    "    \n",
    "    # Summary statistics (excluding NaN values)\n",
    "    valid_rest_days = df['rest_days'].dropna()\n",
    "    \n",
    "    print(f\" Rest days calculated for all teams (within seasons only)\")\n",
    "    print(f\" Off-season gaps excluded: {df['rest_days'].isna().sum()} records\")\n",
    "    print(f\" Valid rest day calculations: {len(valid_rest_days)} records\")\n",
    "    \n",
    "    if len(valid_rest_days) > 0:\n",
    "        rest_stats = valid_rest_days.describe()\n",
    "        print(f\"\\n=== REST DAYS STATISTICS (Season-Aware) ===\")\n",
    "        print(f\"Mean rest days: {rest_stats['mean']:.1f}\")\n",
    "        print(f\"Median rest days: {rest_stats['50%']:.1f}\")\n",
    "        print(f\"Min rest days: {rest_stats['min']:.0f}\")\n",
    "        print(f\"Max rest days: {rest_stats['max']:.0f}\")\n",
    "        \n",
    "        # Count of different rest periods\n",
    "        rest_counts = valid_rest_days.value_counts().sort_index()\n",
    "        print(f\"\\nMost common rest periods:\")\n",
    "        print(rest_counts.head(8).to_string())\n",
    "        \n",
    "        # Season analysis\n",
    "        season_counts = df.groupby('season')['rest_days'].count()\n",
    "        print(f\"\\n=== SEASON BREAKDOWN ===\")\n",
    "        print(\"Valid rest day calculations per season:\")\n",
    "        print(season_counts.to_string())\n",
    "        \n",
    "        # Check for any suspiciously long rest periods (potential issues)\n",
    "        long_rest = valid_rest_days[valid_rest_days > 30]\n",
    "        if len(long_rest) > 0:\n",
    "            print(f\"\\n  Found {len(long_rest)} rest periods > 30 days:\")\n",
    "            long_rest_matches = df[df['rest_days'] > 30][['Date', 'team_name', 'rest_days', 'season']]\n",
    "            print(long_rest_matches.head().to_string(index=False))\n",
    "            print(f\"Note: These may be mid-season breaks or scheduling anomalies\")\n",
    "        else:\n",
    "            print(f\"\\n All rest periods are within reasonable range (‚â§30 days)\")\n",
    "    else:\n",
    "        print(f\"\\n  No valid rest day calculations found\")\n",
    "    \n",
    "    # Drop the temporary season column\n",
    "    df = df.drop('season', axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_travel_distance(team_stats_df):\n",
    "    \"\"\"\n",
    "    Step 4c: Calculate travel distance for away teams\n",
    "    \n",
    "    Uses team home cities and match venues to calculate travel distance.\n",
    "    Only away teams travel, so home teams get 0 distance.\n",
    "    \n",
    "    Args:\n",
    "        team_stats_df (pd.DataFrame): Team-level stats dataframe\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Enhanced dataframe with travel_distance_km column\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n=== STEP 4c: Calculating Travel Distance ===\")\n",
    "    \n",
    "    # NRL team home cities (approximate coordinates)\n",
    "    team_locations = {\n",
    "        'Brisbane Broncos': (-27.4975, 153.0137),  # Brisbane\n",
    "        'North Queensland Cowboys': (-19.2590, 146.8169),  # Townsville  \n",
    "        'Gold Coast Titans': (-28.0167, 153.4000),  # Gold Coast\n",
    "        'New Zealand Warriors': (-36.8485, 174.7633),  # Auckland\n",
    "        'Melbourne Storm': (-37.8136, 144.9631),  # Melbourne\n",
    "        'Canberra Raiders': (-35.2809, 149.1300),  # Canberra\n",
    "        'Sydney Roosters': (-33.8688, 151.2093),  # Sydney\n",
    "        'South Sydney Rabbitohs': (-33.8688, 151.2093),  # Sydney\n",
    "        'St George Illawarra Dragons': (-34.4278, 150.8931),  # Wollongong\n",
    "        'Cronulla-Sutherland Sharks': (-34.0544, 151.1518),  # Cronulla\n",
    "        'Manly Sea Eagles': (-33.7969, 151.2841),  # Manly\n",
    "        'Parramatta Eels': (-33.8176, 151.0032),  # Parramatta\n",
    "        'Penrith Panthers': (-33.7506, 150.6934),  # Penrith\n",
    "        'Wests Tigers': (-33.8688, 151.2093),  # Sydney\n",
    "        'Canterbury Bulldogs': (-33.9173, 151.1851),  # Canterbury\n",
    "        'Newcastle Knights': (-32.9283, 151.7817),  # Newcastle\n",
    "        'Dolphins': (-27.4975, 153.0137),  # Brisbane (Redcliffe)\n",
    "    }\n",
    "    \n",
    "    # Common venue locations\n",
    "    venue_locations = {\n",
    "        'Suncorp Stadium': (-27.4648, 153.0099),  # Brisbane\n",
    "        'Queensland Country Bank Stadium': (-19.2598, 146.8181),  # Townsville\n",
    "        'Cbus Super Stadium': (-28.0024, 153.3992),  # Gold Coast\n",
    "        'AAMI Park': (-37.8255, 144.9816),  # Melbourne\n",
    "        'GIO Stadium Canberra': (-35.2447, 149.1014),  # Canberra\n",
    "        'Allianz Stadium': (-33.8878, 151.2273),  # Sydney\n",
    "        'Accor Stadium': (-33.8474, 151.0616),  # Sydney Olympic Park\n",
    "        'WIN Stadium': (-34.4056, 150.8841),  # Wollongong\n",
    "        'PointsBet Stadium': (-34.0481, 151.1394),  # Cronulla\n",
    "        '4 Pines Park': (-33.7742, 151.2606),  # Manly\n",
    "        'CommBank Stadium': (-33.8007, 150.9810),  # Parramatta\n",
    "        'BlueBet Stadium': (-33.7347, 150.6750),  # Penrith\n",
    "        'Leichhardt Oval': (-33.8821, 151.1589),  # Leichhardt\n",
    "        'McDonald Jones Stadium': (-32.9154, 151.7734),  # Newcastle\n",
    "        'Mt Smart Stadium': (-36.9278, 174.8384),  # Auckland\n",
    "        'Kayo Stadium': (-27.3644, 153.0486),  # Redcliffe\n",
    "    }\n",
    "    \n",
    "    def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "        \"\"\"Calculate distance between two points on Earth using Haversine formula\"\"\"\n",
    "        from math import radians, sin, cos, sqrt, asin\n",
    "        \n",
    "        # Convert to radians\n",
    "        lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "        \n",
    "        # Haversine formula\n",
    "        dlat = lat2 - lat1\n",
    "        dlon = lon2 - lon1\n",
    "        a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "        c = 2 * asin(sqrt(a))\n",
    "        \n",
    "        # Earth's radius in kilometers\n",
    "        r = 6371\n",
    "        \n",
    "        return c * r\n",
    "    \n",
    "    df = team_stats_df.copy()\n",
    "    \n",
    "    # Initialize travel distance column\n",
    "    df['travel_distance_km'] = 0.0\n",
    "    \n",
    "    # Calculate travel distance only for away teams\n",
    "    away_games = df[df['is_home'] == 0].copy()\n",
    "    \n",
    "    for idx, row in away_games.iterrows():\n",
    "        team_name = row['team_name']\n",
    "        venue = row['Venue']\n",
    "        \n",
    "        # Get team home location\n",
    "        if team_name in team_locations:\n",
    "            team_lat, team_lon = team_locations[team_name]\n",
    "        else:\n",
    "            # Default to Sydney for unknown teams\n",
    "            team_lat, team_lon = (-33.8688, 151.2093)\n",
    "        \n",
    "        # Get venue location (try exact match first, then partial match)\n",
    "        venue_lat, venue_lon = None, None\n",
    "        \n",
    "        # Exact match\n",
    "        if venue in venue_locations:\n",
    "            venue_lat, venue_lon = venue_locations[venue]\n",
    "        else:\n",
    "            # Partial match for similar venue names\n",
    "            for venue_key in venue_locations:\n",
    "                if venue_key.lower() in venue.lower() or venue.lower() in venue_key.lower():\n",
    "                    venue_lat, venue_lon = venue_locations[venue_key]\n",
    "                    break\n",
    "        \n",
    "        # Default to team's home city if venue not found\n",
    "        if venue_lat is None:\n",
    "            venue_lat, venue_lon = team_lat, team_lon\n",
    "        \n",
    "        # Calculate distance\n",
    "        distance = haversine_distance(team_lat, team_lon, venue_lat, venue_lon)\n",
    "        df.loc[idx, 'travel_distance_km'] = distance\n",
    "    \n",
    "    print(f\" Travel distances calculated for away games\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    away_distances = df[df['is_home'] == 0]['travel_distance_km']\n",
    "    travel_stats = away_distances.describe()\n",
    "    \n",
    "    print(f\"\\n=== TRAVEL DISTANCE STATISTICS ===\")\n",
    "    print(f\"Mean travel distance: {travel_stats['mean']:.1f} km\")\n",
    "    print(f\"Median travel distance: {travel_stats['50%']:.1f} km\")\n",
    "    print(f\"Max travel distance: {travel_stats['max']:.1f} km\")\n",
    "    print(f\"Min travel distance: {travel_stats['min']:.1f} km\")\n",
    "    \n",
    "    # Show longest travels\n",
    "    longest_travels = df[df['travel_distance_km'] > 0].nlargest(5, 'travel_distance_km')\n",
    "    print(f\"\\nLongest travel distances:\")\n",
    "    travel_display = longest_travels[['Date', 'team_name', 'Venue', 'City', 'travel_distance_km']]\n",
    "    print(travel_display.to_string(index=False))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Assemble Final Model-Ready DataFrame\n",
    "\n",
    "This is the final and most important step. We merge all the engineered team-level features back into our original match-level DataFrame.\n",
    "\n",
    "The key action here is creating **difference features** (e.g., `elo_diff`, `form_margin_diff_5`). Our model will learn from the *relative difference* in form and strength between the two competing teams, which is much more predictive than looking at each team's stats in isolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_final_model_ready_dataframe(df_matches, df_team_features):\n",
    "    print(\"\\n--- Step 5: Assembling Final Model-Ready DataFrame ---\")\n",
    "    \n",
    "    # Split features into home and away sets\n",
    "    home_stats = df_team_features[df_team_features['is_home'] == 1].copy()\n",
    "    away_stats = df_team_features[df_team_features['is_home'] == 0].copy()\n",
    "\n",
    "    # Select and rename feature columns\n",
    "    feature_cols = [col for col in df_team_features.columns if col.startswith(('rolling', 'pre_match', 'rest', 'travel'))]\n",
    "    home_features = home_stats[['match_id'] + feature_cols].add_prefix('home_')\n",
    "    away_features = away_stats[['match_id'] + feature_cols].add_prefix('away_')\n",
    "\n",
    "    # Merge back to the match dataframe\n",
    "    df_final = df_matches.merge(home_features, left_on='match_id', right_on='home_match_id', how='left')\n",
    "    df_final = df_final.merge(away_features, left_on='match_id', right_on='away_match_id', how='left')\n",
    "\n",
    "    # Create difference features\n",
    "    df_final['elo_diff'] = df_final['home_pre_match_elo'] - df_final['away_pre_match_elo']\n",
    "    for window in [3, 5, 8]:\n",
    "        df_final[f'form_margin_diff_{window}'] = df_final[f'home_rolling_avg_margin_{window}'] - df_final[f'away_rolling_avg_margin_{window}']\n",
    "        df_final[f'form_win_rate_diff_{window}'] = df_final[f'home_rolling_win_percentage_{window}'] - df_final[f'away_rolling_win_percentage_{window}']\n",
    "        # ... (add other diffs as needed)\n",
    "    \n",
    "    # Add market features\n",
    "    df_final['home_implied_prob'] = 1 / df_final['Home Odds']\n",
    "    df_final['away_implied_prob'] = 1 / df_final['Away Odds']\n",
    "    df_final['market_spread'] = df_final['home_implied_prob'] - df_final['away_implied_prob']\n",
    "    \n",
    "    print(f\"‚úì Final model-ready DataFrame created. Shape: {df_final.shape}\")\n",
    "    return df_final\n",
    "\n",
    "# Execute Step 5\n",
    "df_model_ready = assemble_final_model_ready_dataframe(df_cleaned, team_stats_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Save Final Datasets\n",
    "\n",
    "The pipeline is complete. We save the final model-ready DataFrame to a new CSV file. This file will be the single source of truth for all subsequent model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Step 6: Saving Final Datasets ---\")\n",
    "\n",
    "output_path = 'data/nrl_matches_final_model_ready.csv'\n",
    "df_model_ready.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"‚úì Final dataset saved successfully to: {output_path}\")\n",
    "print(\"\\nüèÜ Pipeline Complete! The data is now ready for model training. üèÜ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
